# üìö Technical Documentation: AI Powered RAG Chatbot
*n8n + Google Drive + Gemini + Qdrant Cloud*

---

## üöÄ RAG Implementation Approach and Rationale

- **Retrieval-Augmented Generation (RAG)**: This architecture fuses document ingestion from Google Drive, metadata extraction using LLMs (OpenAI, Gemini), embedding generation (OpenAI), and vector storage/retrieval (Qdrant Cloud).
- **Rationale**: RAG empowers the chatbot to deliver contextually relevant, up-to-date answers by grounding responses in user-specific documents, overcoming the limitations of static LLM knowledge.

---

## üß© Challenges Faced and Solutions

| Challenge | Solution |
|---|---|
| **Document Ingestion** | Automated n8n workflow triggers and batch processing for real-time Google Drive updates. |
| **Vector Store Consistency** | Human-in-the-loop approval via Telegram for deletions/updates, ensuring data integrity. |
| **Metadata Extraction** | Custom prompt engineering for LLM-based extraction, improving accuracy and reliability. |

---

## üõ†Ô∏è n8n Workflow Design Decisions

- **Modular Node Structure**: Separate nodes for ingestion, extraction, embedding, storage, and retrieval, ensuring maintainability and scalability.
- **Sticky Notes**: Embedded documentation within the workflow for clarity and onboarding.
- **Human-in-the-Loop**: Telegram integration for approval of critical operations (e.g., vector deletions), enhancing safety.
- **Multi-LLM Support**: Both Gemini and OpenAI are integrated for flexibility and fallback, reducing downtime and vendor lock-in.

---

## ‚ö° Performance Considerations

- **Batch Processing & Chunking**: Efficiently processes large documents for embedding, optimizing throughput.
- **Qdrant Cloud**: Chosen for its scalable, low-latency vector search capabilities, supporting rapid retrieval even at scale.
- **Adjustable Parameters**: Chunk size and top-K retrieval are configurable, allowing fine-tuning for speed vs. accuracy.

---

## üìà Ideas for Scalability and Improvements

- **Automated Scaling**: Dynamically scale Qdrant Cloud and n8n instances to handle increased load.
- **Multi-Source Support**: Extend ingestion to Dropbox, OneDrive, and other sources for broader coverage.
- **Query Caching**: Implement caching for frequent queries to reduce latency and API costs.
- **Monitoring & Alerting**: Add workflow health checks and alerting for failures to ensure reliability.

---

## üßë‚Äçüíª Code Components

### API Integrations

- **Google Drive**: Automated document ingestion and storage.
- **Google Docs**: Updates chat history for transparency and traceability.
- **OpenAI**: Handles both embeddings and chat generation for robust NLP.
- **Google Gemini (PaLM)**: Used for chat and metadata extraction, providing LLM diversity.
- **Qdrant Cloud**: Vector storage and retrieval, enabling semantic search.
- **Telegram**: Manages approval/notifications for critical workflow steps.

### Vector DB Setup Scripts

- **Qdrant Cloud Collection**: Setup and management handled via n8n nodes and custom scripts; collection name is configurable (e.g., `nostr-damus-user-profiles`).

---

## üìù Example Workflow Structure

| Step | n8n Node(s) Used | Purpose |
|---|---|---|
| Data Ingestion | Google Drive Trigger, HTTP | Detect and fetch new/updated files |
| Text Processing | Function, Text Splitter | Extract and chunk text |
| Embedding | OpenAI, Gemini | Generate vector representations |
| Vector Storage | Qdrant Cloud | Store and retrieve embeddings |
| Query Handling | Webhook, UI Trigger | Receive user questions |
| Retrieval | Qdrant Vector Search | Find relevant context |
| Generation | OpenAI, Gemini | Generate final answer |
| Approval/Alerts | Telegram | Human-in-the-loop for critical actions |

---
